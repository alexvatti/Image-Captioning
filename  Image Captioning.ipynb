{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c080fc63",
   "metadata": {},
   "source": [
    "# Objective Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf0c92",
   "metadata": {},
   "source": [
    "![](Image-Caption.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1584c",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde5154",
   "metadata": {},
   "source": [
    "## 1.1 Glove - WORD2VEC -300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "!unzip glove.42B.300d.zip\n",
    "!rm glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b75bc3",
   "metadata": {},
   "source": [
    "## 1.2 Images\n",
    "\n",
    "- ***In kaggle - Search for \"alexreddy/images-caption\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602a7e1",
   "metadata": {},
   "source": [
    "![](Images-Download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff58f9",
   "metadata": {},
   "source": [
    "## 1.3 Image Caption CSV\n",
    "\n",
    "- ***image_caption_map.csv***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a7a83",
   "metadata": {},
   "source": [
    "# 2. Import the Necessary Pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow.keras.preprocessing.image as tf_image\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow.keras.applications.inception_v3 as inception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160296f",
   "metadata": {},
   "source": [
    "# 3. Inceptionv3 - Transfer learning - download weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_model = InceptionV3(weights='imagenet')\n",
    "encode_model = Model(encode_model.input, encode_model.layers[-2].output)\n",
    "WIDTH = 299\n",
    "HEIGHT = 299\n",
    "OUTPUT_DIM = 2048\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 10\n",
    "preprocess_input = inception.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17c6f7",
   "metadata": {},
   "source": [
    "# 4. Image Encode - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ef430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeImage(img):\n",
    "    img = img.resize((WIDTH, HEIGHT))\n",
    "    x = tf_image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    x = encode_model.predict(x) # Get the encoding vector for the image\n",
    "    x = np.reshape(x, OUTPUT_DIM )\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda72f1f",
   "metadata": {},
   "source": [
    "## **image_dir path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '/content/drive/MyDrive/collab_mount/image captioning/images/'\n",
    "image_filename = '1.jpg'\n",
    "image_path = os.path.join(image_dir, image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab70748",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=tf_image.load_img(image_path, target_size=(299,299))\n",
    "encodeImage(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2019e",
   "metadata": {},
   "source": [
    "# 5. Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/content/drive/MyDrive/collab_mount/image captioning/image_caption_map.csv')\n",
    "print(data.head(2))\n",
    "data['caption']=data['caption'].apply(lambda x:START+' '+x+' '+STOP)\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0c065",
   "metadata": {},
   "source": [
    "# 6. Process the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_these=[]\n",
    "encoded_images={}\n",
    "for i in range(data.shape[0]):\n",
    "    image_path='/content/drive/MyDrive/collab_mount/image captioning/images/'+data['file_name'][i]\n",
    "    print(image_path)\n",
    "    try:\n",
    "        img = tf_image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "        encoded_images[int(data['file_name'][i].split('.')[0])] = encodeImage(img)\n",
    "    except:\n",
    "        print('remove: ',i)\n",
    "        remove_these.append(data['file_name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea48f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(remove_these))\n",
    "data=data[-data['file_name'].isin(remove_these)]\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242209d",
   "metadata": {},
   "source": [
    "![](Images-Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data['id']=[int(data['file_name'][i].split('.')[0]) for i in range(data.shape[0])]\n",
    "print(len(encoded_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377897c",
   "metadata": {},
   "source": [
    "# 7. clean up captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdac881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['caption']=data['caption'].apply(lambda x:re.sub(\"[\"+punctuation+\"]\",' ',x))\n",
    "data['caption']=data['caption'].apply(lambda x:re.sub(\"\\d\",' ',x))\n",
    "data['caption']=data['caption'].apply(lambda x:re.sub(\"\\s+\",' ',x))\n",
    "data['caption']=data['caption'].str.lower()\n",
    "print(data['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becffe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_threshold = 5\n",
    "word_counts = {}\n",
    "for caption in data['caption']:\n",
    "    for w in word_tokenize(caption):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e90d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_lens=[]\n",
    "for caption in data['caption']:\n",
    "    words=word_tokenize(caption)\n",
    "    words=[w for w in words if w in vocab]\n",
    "    caption_lens.append(len(words))\n",
    "max_length=max(caption_lens)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acd75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(idxtoword) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb3c97",
   "metadata": {},
   "source": [
    "# 8. Word 2 Vec - Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_index = {}\n",
    "f = open( 'glove.42B.300d.txt', encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    line=line.strip()\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4dac3",
   "metadata": {},
   "source": [
    "# 9. LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98397382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization,add\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, encoded_images, wordtoidx, max_length, num_photos_per_batch):\n",
    "  # x1 - Training data for photos\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "    x1, x2, y = [], [], []\n",
    "    n=0\n",
    "    while True:\n",
    "        for k,caption in enumerate(data['caption']):\n",
    "        n+=1\n",
    "        photo = encoded_images[data['id'][k]]\n",
    "      # Each photo has 5 descriptions\n",
    "        seq = [wordtoidx[word] for word in word_tokenize(caption) if word in wordtoidx]\n",
    "        # Generate a training case for every possible sequence and outcome\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            x1.append(photo)\n",
    "            x2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "        if n==num_photos_per_batch:\n",
    "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
    "        # next when generator gets called iteration will start from where we left off\n",
    "        # this makes it make a pass through the complete data in an epoch\n",
    "        x1, x2, y = [], [], []\n",
    "        n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoded_images.keys()))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d9c30",
   "metadata": {},
   "source": [
    "# 10. LSTM Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a97412",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbc118",
   "metadata": {},
   "source": [
    "# 11. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.optimizer.lr = 1e-4\n",
    "number_pics_per_batch = 10\n",
    "steps = len(data['caption'])//number_pics_per_batch\n",
    "\n",
    "for i in tqdm(range(EPOCHS)):\n",
    "    generator = data_generator(data, encoded_images, wordtoidx, max_length, number_pics_per_batch)\n",
    "    caption_model.fit_generator(generator, epochs=10, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8552de",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.save_weights('caption_model-1.hdf5')\n",
    "caption_model.save('caption_model-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605e186",
   "metadata": {},
   "source": [
    "## 12. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCaption(photo):\n",
    "    in_text = START\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idxtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=5  #79\n",
    "image_file=data.iloc[index,2]\n",
    "\n",
    "image=encoded_images[int(image_file.split('.')[0])]\n",
    "\n",
    "image = image.reshape((1,OUTPUT_DIM))\n",
    "x=plt.imread('/content/drive/MyDrive/collab_mount/image captioning/images/'+image_file)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Caption:\",generateCaption(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=287 #79\n",
    "image_file=data.iloc[index,2]\n",
    "\n",
    "image=encoded_images[int(image_file.split('.')[0])]\n",
    "\n",
    "image = image.reshape((1,OUTPUT_DIM))\n",
    "x=plt.imread('/content/drive/MyDrive/collab_mount/image captioning/images/'+image_file)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Caption:\",generateCaption(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=185  #79\n",
    "image_file=data.iloc[index,2]\n",
    "\n",
    "image=encoded_images[int(image_file.split('.')[0])]\n",
    "\n",
    "image = image.reshape((1,OUTPUT_DIM))\n",
    "x=plt.imread('/content/drive/MyDrive/collab_mount/image captioning/images/'+image_file)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Caption:\",generateCaption(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=549  #79\n",
    "image_file=data.iloc[index,2]\n",
    "\n",
    "image=encoded_images[int(image_file.split('.')[0])]\n",
    "\n",
    "image = image.reshape((1,OUTPUT_DIM))\n",
    "x=plt.imread('/content/drive/MyDrive/collab_mount/image captioning/images/'+image_file)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Caption:\",generateCaption(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ecea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=79\n",
    "image_file=data.iloc[index,2]\n",
    "\n",
    "image=encoded_images[int(image_file.split('.')[0])]\n",
    "\n",
    "image = image.reshape((1,OUTPUT_DIM))\n",
    "x=plt.imread('/content/drive/MyDrive/collab_mount/image captioning/images/'+image_file)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Caption:\",generateCaption(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
